{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"gymnasium[box2d]\"\n",
    "# !pip install \"gymnasium[other]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from base64 import b64encode\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()\n",
    "print(\"CUDA доступна:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "def render_mp4(videopath: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для рендеринга видео в формате mp4.\n",
    "    Args:\n",
    "    videopath - Путь к файлу с видео.\n",
    "    \"\"\"\n",
    "    \n",
    "    mp4 = open(videopath, 'rb').read()\n",
    "    base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "    return f'<video width=600 controls><source src=\"data:video/mp4; base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Класс для хранения и выборки данных из буфера\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):      \n",
    "        # deque - инициализация двусторонней очереди\n",
    "        # capacity - макс. количество элементов, которые могут быть сохранены в буфер\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # добавление элементов в буфер\n",
    "        self.memory.append(transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # выбор элементов из буфера\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        # n_observations: количество наблюдений\n",
    "        # n_actions: количество действий\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x) # возвращает тензор tensor([[left0exp,right0exp]...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE - количество эпизодов, отобранных из буфера воспроизведения\n",
    "# GAMMA - коэффициент дисконтирования\n",
    "# EPS_START - начальное значение эпсилон\n",
    "# EPS_END - конечное значение эпсилон\n",
    "# EPS_DECAY - скорость экспоненциального спада эпсилон, чем больше - тем медленнее падение\n",
    "# TAU - скорость обновления целевой сети\n",
    "# LR - скорость обучения оптимизатора ``AdamW``.\n",
    "\n",
    "BATCH_SIZE = 128   # количество эпизодов, отобранных из буфера воспроизведения\n",
    "GAMMA = 0.995       # коэффициент дисконтирования\n",
    "EPS_START = 0.9    # начальное значение эпсилон\n",
    "EPS_END = 0.05     # конечное значение эпсилон\n",
    "EPS_DECAY = 10000   # скорость экспоненциального спада эпсилон, чем больше - тем медленнее падение\n",
    "TAU = 0.005        # скорость обновления целевой сети\n",
    "LR = 5e-5          # скорость обучения оптимизатора `AdamW`\n",
    "FULL_MEMORY_LENGTH = 1000000   # длинна памяти\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    # wind_power = 5.0,\n",
    "    # turbulence_power = 1.1,\n",
    "    max_episode_steps=2000,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"video\", name_prefix=f\"LunarLander\", episode_trigger=lambda ep: ep % 50 == 0)  # , episode_trigger=lambda e: True\n",
    "\n",
    "# Получить число действий\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Получить число степеней свободы состояний\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициилизировать сети: целевую и политики\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузить в целевую сеть коэффициенты из сети политики\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Задать оптимайзер\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Инициализировать Replay Memory buffer\n",
    "memory = ReplayMemory(FULL_MEMORY_LENGTH)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# массив длительности эпизода - пойдет в отчет о том, сколько продержался агент\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done  # глобальный шаги для убывания epsilon\n",
    "    #  случайное значение для определения какой шаг будем делать жадный или случайный\n",
    "    sample = random.random()\n",
    "    \n",
    "    # установка порога принятия решения - уровня epsilon\n",
    "    # (функция экспоненциального затухания случайности жадной стратегии)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    \n",
    "    # увеличиваем счетчик шагов\n",
    "    steps_done += 1\n",
    "    \n",
    "    # если случайный порог больше epsilon-порога\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) вернет наибольшее значение столбца в каждой строке.\n",
    "            # Второй столбец в результате max - это индекс того места, \n",
    "            # где был найден максимальный элемент, \n",
    "            # поэтому мы выбираем действие с наибольшим ожидаемым вознаграждением.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # Иначы выбираем случайное дайствие\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    #  преобразуем массив длительностей в тензор\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf();  # Очищаем график\n",
    "        plt.title('Training...')\n",
    "        \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Отрисовываем среднюю оценку за 100 эпизодов\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(batch_size: int):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Получить из памяти батч\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Преобразовать его в namedtuple\n",
    "    batch = transition(*zip(*transitions))\n",
    "\n",
    "    # Вычислить маску нефинальных состояний и соединить элементы батча\n",
    "    # (финальным состоянием должно быть то, после которого моделирование закончилось)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    # Собираем батчи для состояний, действий и наград\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Вычислить Q(s_t, a) - модель вычисляет Q(s_t), \n",
    "    # затем мы выбираем столбцы предпринятых действий. \n",
    "    # Это те действия, которые были бы предприняты для каждого состояния партии в соответствии с policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Вычислить V(s_{t+1}) для всех следующих состояний.\n",
    "    # Ожидаемые значения действий для не_финальных_следующих_состояний вычисляются \n",
    "    # на основе \"старшей\" целевой_сети; выбирается их наилучшее вознаграждение с помощью max(1)[0].\n",
    "    # Это объединяется по маске, так что мы будем иметь либо ожидаемое значение состояния, \n",
    "    # либо 0, если состояние было финальным.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Вычисляем ожидаемые Q значения\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Объединяем все в общий лосс\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Готовим градиент\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Обрезаем значения градиента - проблемма исчезающего/взрывающего градиента\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 100\n",
    "else:\n",
    "    num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Для каждого эпизода инициализируем начальное состояние\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    # выполняем действия пока не получим флаг done\n",
    "    # t - считает сколько шагов успели сделать пока шест не упал\n",
    "    # for t in count():\n",
    "    for t in range(2000):\n",
    "        # выбираем действие [0, 1]\n",
    "        action = select_action(state)\n",
    "        # Делаем шаг\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        \n",
    "        # Преобразуем в тензор\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Объединяем done по двум конечным состояниям\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # присваиваем следующее состояние\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # отправляем в память\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # переходим на следующее состояние\n",
    "        state = next_state\n",
    "\n",
    "        # запускаем обучение сети\n",
    "        optimize_model(batch_size=128)\n",
    "\n",
    "        # делаем \"мягкое\" обновление весов\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        \n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        # Если получили terminated or truncated завершаем эпизод обучения\n",
    "        if done:\n",
    "            # добавляем в массив продолжительность эпизода\n",
    "            episode_durations.append(t + 1)\n",
    "            # отрисовываем график\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "# отрисовываем финальный график\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
